{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "from pm4py.objects.log.obj import EventLog, Trace, Event\n",
    "import math\n",
    "import csv\n",
    "from bidict import bidict\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import BatchNormalization\n",
    "from pre_process import pre_processing\n",
    "from pre_process import preprocess_graph_attributes\n",
    "from utility import utility_functions\n",
    "from utility import constants\n",
    "from mappers import mappers\n",
    "from feature_encoding import vectorize_features\n",
    "from train_and_test import train\n",
    "from train_and_test import prediction\n",
    "from evaluation import post_processing\n",
    "from evaluation import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ea317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and convert the dataset \n",
    "\n",
    "\n",
    "# Import the required attributes from the CSV file\n",
    "df = pd.read_csv('.//logistics_format_CSS9J_TPJB2.csv')[constants.required_attributes+constants.event_attribute_features+constants.case_attribute+constants.src_dest_attributes]\n",
    "\n",
    "# Convert the DataFrame into the event log structure\n",
    "log = xes_converter.apply(df, variant=xes_converter.Variants.TO_EVENT_LOG) \n",
    "\n",
    "# Convert the timestamp from string to datetime format\n",
    "log = pre_processing.convert_timestamp(log)\n",
    "\n",
    "# Adjust the arrived time\n",
    "log = pre_processing.adjust_arrived_time(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "global variables using the entire event log\n",
    "\"\"\"\n",
    "# Calculate the average dwell time in seconds: 245.68552766191863\n",
    "average_dwell_time_in_sec = utility_functions.average_dwell_time(log)\n",
    "\n",
    "# Calculate the average time since case start in seconds: 5737.639253954513\n",
    "average_time_since_start_in_sec = utility_functions.average_time_since_case_start(log)\n",
    "\n",
    "# Build the mapper for average dwell time based on EQTYP: \n",
    "dwell_time_mapper = mappers.build_average_dwell_time_mapper(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fold creation\n",
    "\"\"\"\n",
    "# Create folds for training and testing\n",
    "folds = pre_processing.create_folds(log, 3)\n",
    "\n",
    "# get the training log from first two folds\n",
    "training_log = EventLog([case for sub_log in folds[:2] for case in sub_log])\n",
    "\n",
    "# Use the third fold for testing\n",
    "testing_log = EventLog([case for sub_log in folds[2:] for case in sub_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global variables using only the training log\n",
    "\"\"\"\n",
    "# Build the mapper for EID based on the training log\n",
    "eid_mapper = mappers.build_eid_mapper(training_log)\n",
    "\n",
    "# Get the maximum case length in the training log and add an end signal\n",
    "max_case_len = max([len(case) for case in training_log]) + 1\n",
    "\n",
    "# Build mappers for event attribute features based on the training log\n",
    "event_attribute_mappers = {attr: mappers.build_event_attr_mapper(training_log, attr) for attr in constants.event_attribute_features}\n",
    "\n",
    "# Build mappers for case attribute features based on the training log\n",
    "case_attribute_mappers = {attr: mappers.build_case_attr_mapper(training_log, attr) for attr in constants.case_attribute_features}\n",
    "\n",
    "# Get the routing netwrok of logistic log\n",
    "network = preprocess_graph_attributes.preprocess_graph_attributes('.//subgraph_CSS9J_TPJB2.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model with training log\n",
    "\n",
    "\n",
    "# Vectorize the features for training\n",
    "X = vectorize_features.vectorize_features(training_log, average_time_since_start_in_sec, eid_mapper, dwell_time_mapper, event_attribute_mappers, case_attribute_mappers, max_case_len, network)\n",
    "\n",
    "# Vectorize the expected EID predictions for training\n",
    "equipment_prediction = vectorize_features.vectorize_equipment_prediction(training_log, eid_mapper) \n",
    "\n",
    "# Vectorize the expected dwell time predictions for training\n",
    "time_prediction = vectorize_features.vectorize_time_prediction(training_log, dwell_time_mapper) \n",
    "\n",
    "# Train the model using the vectorized features and predictions\n",
    "train.train(X, equipment_prediction, time_prediction, max_case_len, constants.models_path, num_features=len(X[0][0]),number_of_eids=len(eid_mapper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with testing log\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "# Load the trained model\n",
    "model = load_model(constants.models_path+'model_70-0.67.h5')\n",
    "\n",
    "# Set the starting prefix size for prediction\n",
    "starting_prefix_size = 2\n",
    "\n",
    "# Perform predictions on the testing log using the loaded model\n",
    "all_prediction = prediction.predict(testing_log, model, average_time_since_start_in_sec, eid_mapper, dwell_time_mapper, event_attribute_mappers, case_attribute_mappers, max_case_len, network, starting_prefix_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prefix log based on the testing log with a specified prefix size\n",
    "prefix_log = post_processing.create_prefix_log(testing_log, starting_prefix_size)\n",
    "\n",
    "# Perform post-processing on the prefix log using the predicted results\n",
    "prediction_log = post_processing.post_process(prefix_log, all_prediction)\n",
    "\n",
    "# Add ground truth information to the predicted events in the prediction log\n",
    "prediction_log = post_processing.add_ground_truth(testing_log, prediction_log)\n",
    "\n",
    "# Convert the prediction log to a DataFrame and save it to a CSV file\n",
    "result_with_ground_truth = xes_converter.apply(prediction_log, variant=xes_converter.Variants.TO_DATA_FRAME)\n",
    "pm4py.write_xes(log, 'result_log_CSS9J_TPJB2.xes')\n",
    "result_with_ground_truth.to_csv('result_CSS9J_TPJB2.csv')\n",
    "# Calculate the accuracy of EID prediction\n",
    "accuracy_of_equipment_prediction = evaluation.calculate_accuracy(result_with_ground_truth, len(prediction_log), starting_prefix_size)\n",
    "\n",
    "# Calculate MAE and MAPE for dwell time prediction\n",
    "error_mappers = evaluation.calculate_mae_mape(prediction_log, starting_prefix_size)\n",
    "\n",
    "# Print the MAE and MAPE for timestamp prediction\n",
    "print(\"MAE for next timestamp prediction\")\n",
    "print(error_mappers['MAE'])\n",
    "print(\"MAPE for next timestamp prediction\")\n",
    "print(error_mappers['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18f452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
